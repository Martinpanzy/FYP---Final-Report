\chapter{Conclusion}

This project successfully addressed the precise AI planning and control problem - putting a shoelace on a shoe using bimanual robot, YuMi, and an external camera. 

Recall Figure \ref{workflow}, the system workflow, the core project which is passing the shoelace into a hole (with $4mm$ radius) has been successfully completed. In addition, YuMi equips with capabilities to adjust the pose of the shoe if necessary, as well as grab and pull the shoelace out to proper location for further manipulation after insertion. The corresponding success rates can be found in Table \ref{sris}.

\begin{table}[H]
\centering
\begin{tabular}{||c|c|c|c||}
\hline
Detection & Adjustment & Insertion & Pulling \\ \hline\hline
100\% & 95\% & 87\% & 91\% \\ \hline
\end{tabular}
\caption{The success rate of integrated system}
\label{sris}
\end{table}

For computer vision part, all the requirements were met. The two cameras, ASUS Xtion and ZED Mini, were correctly set up and calibrated. All computer vision algorithms in this project were implemented for both of them. As discussed in Section \ref{shoedetection}, the shoe can be successfully detected by employing YOLO, regardless of whether there are other interfering objects on the workbench. The calculation of required locations for shoe pose adjustment is introduced in Section \ref{shoeadjust}. The location conversion between camera frame and YuMi frame is based on ROS $TF$ topic. Real-time 6D pose (3D location and 3D orientation) of shoe hole can also be estimated with high precision. The location is calculated based on some image processing techniques including color detection, blurred filter etc, details can be found in Section \ref{3dlocationestimation}. While the method to compute accurate orientation of the shoe hole is mentioned in Section \ref{3DOrientationofShoeHole}, which is based on RANSAC and pre-defined constraints.

The advantages of my vision approaches are fast, relatively stable, and only require one camera. Both the normal vector of the surrounding plane of the shoe hole and color detection have great volatility in the beginning. By extracting ...

For motion planning part, every functionality has been fulfilled as well. My algorithms can successfully plan and move YuMi for shoe and shoelace manipulation while avoiding potential collisions with the environments. The important details about interface and planning scene setup are told in Section \ref{motionplansetup}. The movement control, and related implementation methods such as setting joint goal and pose goals are discussed in Section \ref{movementcontrol}. The three safety poses throughout manipulation process are defined in Section \ref{safetyposescalculation}. Detailed shoe pose adjustment plan, shoelace insertion plan, shoelace grabbing plan and a series of their movement photo examples can be found in Section \ref{adj}, \ref{approachposegripper}, and \ref{shoelacegrabbing} respectively. The accurate offset adjustment approach is in Section \ref{offsetadjustment}.


Back to the workflow, Figure \ref{workflow}, the last two parts: "compute the orientation of the shoelace" and "align the gripper with the shoelace and reclamp" have not been implemented yet. However, the methods are no different from what have been described in the report. The former can be achieved by using color detection and RANSAC. According to the approaches introduced in Section \ref{approachposegripper}, YuMi can align its right gripper with the colored head of shoelace and accomplish reclamping. Adjusting shoe pose using single arm can refer to Section \ref{adj}. After that, the whole workflow is completed and the shoelace can be passed into every hole of the shoe.



%critical evaluation compare to previous ....product, algorithm, ...

 %- design choice 
 %- how overcome 
 %- what did I learn

%advantages (positively and worthwhile) achievement!!

%disadvantages (limit)
